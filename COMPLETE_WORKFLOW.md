# Complete Workflow: From SLURM Export to Analysis

## Current Situation

Your existing file `oscar_all_jobs_2025.csv` contains:
- ‚úÖ Job resource data (CPUs, memory, nodes, timing)
- ‚ùå **Missing: User and Group information**

The anonymization script **cannot add user/group data** - it only anonymizes data that's already in the CSV.

## What You Need to Do

You need to **re-export from SLURM** with user/group columns included.

---

## Complete Step-by-Step Process

### Step 1: Export Data with User/Group Information

**On the SLURM cluster as admin:**

```bash
# Use the provided export script
./export_with_users.sh
```

This runs:
```bash
sacct -a \
  --format=User,Account,Group,ReqCPUS,ReqMem,NNodes,NodeList,AllocTRES,ReqGRES,Submit,Start,End \
  --starttime 2024-01-01 \
  --parsable2 \
  > oscar_jobs_with_users_YYYYMMDD.csv
```

**What this exports:**
- `User` - Username (e.g., jsmith)
- `Account` - SLURM account
- `Group` - Unix group
- `ReqCPUS` - CPUs requested (your `cpus_req`)
- `ReqMem` - Memory requested (your `mem_req`)
- `NNodes` - Nodes allocated (your `nodes_alloc`)
- `NodeList` - Node names (your `nodelist`)
- `AllocTRES` - Allocated resources (your `tres_alloc`)
- `ReqGRES` - Generic resources like GPUs (your `gres_used`)
- `Submit` - Submission time (your `submit_time`)
- `Start` - Start time (your `start_time`)
- `End` - End time (your `end_time`)

**Result:** You now have a CSV with **all your existing data PLUS user/group columns**.

---

### Step 2: Verify the Export

```bash
# Check the file looks correct
head oscar_jobs_with_users_YYYYMMDD.csv

# Check how many jobs
tail -n +2 oscar_jobs_with_users_YYYYMMDD.csv | wc -l

# Sample a few users (should see real usernames)
tail -n +2 oscar_jobs_with_users_YYYYMMDD.csv | cut -d'|' -f1 | sort -u | head -20
```

You should see real usernames in the first column.

---

### Step 3: Run Anonymization

```bash
./anonymize_cluster_data.sh \
  oscar_jobs_with_users_YYYYMMDD.csv \
  oscar_jobs_anonymized.csv \
  mapping_secure.txt
```

**What this does:**
- Reads the CSV with real user/group names
- Creates anonymous IDs:
  - `jsmith` ‚Üí `user_0001`
  - `mjones` ‚Üí `user_0002`
  - `physics` ‚Üí `group_A`
  - `biology` ‚Üí `group_B`
- Writes anonymized CSV (safe to share)
- Writes mapping file (keep secure!)

---

### Step 4: Secure the Mapping File

```bash
# Set restrictive permissions (only you can read)
chmod 600 mapping_secure.txt

# Move to secure admin-only location
sudo mkdir -p /root/secure
sudo mv mapping_secure.txt /root/secure/cluster_mapping_$(date +%Y%m%d).txt

# Optionally encrypt it
cd /root/secure
sudo gpg -c cluster_mapping_$(date +%Y%m%d).txt
sudo shred -u cluster_mapping_$(date +%Y%m%d).txt
```

**CRITICAL:** The mapping file allows de-anonymization. Protect it like you would the original data.

---

### Step 5: Share Anonymized Data

The file `oscar_jobs_anonymized.csv` can now be safely shared:
- With external consultants (like me!)
- For institutional analysis
- For research purposes

It contains:
- ‚úÖ All job resource patterns
- ‚úÖ Per-user patterns (as anonymous IDs)
- ‚úÖ Per-group patterns (as anonymous IDs)
- ‚ùå No real identities

---

## What Can Be Analyzed

With the anonymized data including user/group fields, we can:

### Infrastructure Optimization ($7.4M/year savings)
- ‚úÖ Already demonstrated with your existing data
- Cloud bursting strategy (85th percentile sizing)
- Throughput vs capacity analysis

### User-Level Optimization ($700K-$1.8M/year additional savings)
These require user/group data:

1. **Power User Optimization**
   - Identify top 10 users (typically 70-80% of usage)
   - Optimize their workflows specifically
   - Expected: $500K-$1.2M/year

2. **Waste Attribution**
   - Identify users requesting MAX_INT memory
   - Target training to those specific users
   - Expected: $100K-$400K/year

3. **Group-Level Strategies**
   - Different groups have different needs (physics vs ML vs bio)
   - Tailor resources and policies per group
   - Expected: $50K-$150K/year

4. **Idle Job Analysis**
   - Find users with consistently low CPU efficiency
   - Targeted intervention
   - Expected: $50K-$100K/year

---

## Comparison: What You Have vs What You Need

### Your Current Data (oscar_all_jobs_2025.csv)
```csv
cpus_req,mem_req,nodes_alloc,nodelist,tres_alloc,gres_used,submit_time,start_time,end_time
4,16384,1,node0001,cpu=4,gpu:0,2024-09-30 14:30:00,2024-09-30 14:31:00,2024-09-30 15:30:00
```

**Limitations:**
- ‚ùå Can't identify power users
- ‚ùå Can't attribute waste
- ‚ùå Can't do group-level analysis
- ‚ùå Can't track user behavior over time

### What You Need (oscar_jobs_with_users_YYYYMMDD.csv)
```csv
User,Account,Group,ReqCPUS,ReqMem,NNodes,NodeList,AllocTRES,ReqGRES,Submit,Start,End
jsmith,physics,physics-group,4,16384,1,node0001,cpu=4,gpu:0,2024-09-30 14:30:00,2024-09-30 14:31:00,2024-09-30 15:30:00
```

**Before anonymization** (stays on your secure cluster)

### After Anonymization (oscar_jobs_anonymized.csv)
```csv
User,Account,Group,ReqCPUS,ReqMem,NNodes,NodeList,AllocTRES,ReqGRES,Submit,Start,End
user_0001,group_A,group_A,4,16384,1,node0001,cpu=4,gpu:0,2024-09-30 14:30:00,2024-09-30 14:31:00,2024-09-30 15:30:00
```

**After anonymization** (safe to share for analysis)

**Capabilities:**
- ‚úÖ Can identify power users (as user_0001, user_0002, etc.)
- ‚úÖ Can attribute waste to specific users
- ‚úÖ Can do group-level analysis (group_A vs group_B)
- ‚úÖ Can track behavior patterns
- ‚úÖ No privacy concerns

---

## Timeline

**If you start today:**

| Week | Activity | Outcome |
|------|----------|---------|
| Week 1 | Present to IT leadership | Approval to proceed |
| Week 2 | Export data with user fields | Have data ready |
| Week 2 | Run anonymization | Safe dataset created |
| Week 3 | Run analyses | Identify $700K-$1.8M in additional opportunities |
| Week 4 | Implement quick wins | Start saving $15K-$20K/month |
| Month 3 | Full implementation | Realize $100K-$200K savings |
| Year 1 | Complete rollout | $7.4M infrastructure + $700K-$1.8M user optimization |

---

## FAQs

### Q: Can't we just analyze what we have?
**A:** We already did! That's the $7.4M infrastructure savings. User/group data unlocks an additional $700K-$1.8M.

### Q: Why do we need to re-export? Can't we join the data somehow?
**A:** SLURM doesn't store permanent job IDs that match across queries. The only way to get user info is to export it in the same query.

### Q: Is this safe?
**A:** Yes. Anonymization is standard practice at Harvard, MIT, Stanford, TACC, NERSC, and every major HPC center. The mapping file stays secure with you.

### Q: What if someone figures out who user_0001 is?
**A:** For most analyses, it doesn't matter. The goal is to identify *patterns* (e.g., "user_0001 requests MAX_INT memory"). You can then use your secure mapping to contact them directly for optimization.

### Q: Can we do this analysis internally without sharing data?
**A:** Absolutely! The anonymization isn't required for internal-only analysis. It's only needed if you want to share data externally. But it's good practice regardless.

---

## Next Steps

1. **Present to leadership** using `EXECUTIVE_SUMMARY_1PAGE.md`
2. **Get approval** from security/legal using `INSTITUTIONAL_APPROVAL_CASE.md`
3. **Run export** using `export_with_users.sh`
4. **Anonymize data** using `anonymize_cluster_data.sh`
5. **Run analysis** to identify the $700K-$1.8M opportunities
6. **Implement improvements** and start saving

---

## Support Files in This Directory

| File | Purpose |
|------|---------|
| `export_with_users.sh` | ‚≠ê **Export SLURM data with user/group fields** |
| `anonymize_cluster_data.sh` | ‚≠ê **Anonymize the exported data** |
| `ANONYMIZATION_README.md` | Detailed anonymization documentation |
| `EXECUTIVE_SUMMARY_1PAGE.md` | Present to leadership |
| `INSTITUTIONAL_APPROVAL_CASE.md` | Comprehensive business case |
| `analyze_mock_user_group_data.py` | Example of what you'll get with user data |
| `MOCK_ANALYSIS_SUMMARY.md` | Sample results with user/group analysis |

---

## Summary

**The key point:** Your current data lacks user/group information. The anonymization script doesn't add this - you need to re-export from SLURM using `export_with_users.sh`, then anonymize using `anonymize_cluster_data.sh`.

**The value:** This unlocks an additional $700K-$1.8M/year in optimization opportunities beyond the $7.4M infrastructure savings already identified.

**The process:** Export ‚Üí Anonymize ‚Üí Analyze ‚Üí Optimize ‚Üí Save millions üí∞
