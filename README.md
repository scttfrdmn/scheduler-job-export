# HPC Multi-Scheduler Data Export & Analysis Toolkit

**Production-ready tools for exporting, anonymizing, and analyzing job data from all major HPC schedulers.**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Schedulers: 5](https://img.shields.io/badge/Schedulers-5-blue.svg)](#supported-schedulers)
[![Documentation: 40k+ words](https://img.shields.io/badge/Docs-40k%2B%20words-green.svg)](#documentation)

---

## üéØ Purpose

This toolkit enables comprehensive behavioral analysis of HPC cluster usage by:

1. **Exporting** job history and cluster configuration from any major HPC scheduler
2. **Anonymizing** user data while preserving usage patterns
3. **Analyzing** submission behavior, utilization, and cloud bursting opportunities

**Primary Use Case:** Test the hypothesis that users avoid submitting jobs during periods of high cluster congestion ("submission abandonment").

---

## ‚ú® Key Features

### üîÑ Universal Scheduler Support

Export data from **all major HPC schedulers** with standardized output:

| Scheduler | Job Export | Cluster Config | Status |
|-----------|------------|----------------|--------|
| **SLURM** | ‚úÖ | ‚úÖ | Production |
| **LSF** (IBM Spectrum) | ‚úÖ | ‚úÖ | Production |
| **PBS/Torque/PBS Pro** | ‚úÖ | ‚úÖ | Production |
| **UGE/SGE/OGE** | ‚úÖ | ‚úÖ | Production |
| **HTCondor** | ‚úÖ | ‚úÖ | Production |

All exporters produce **identical CSV format** for seamless analysis.

### üîí Privacy-Preserving Anonymization

- Deterministic mapping: `jsmith` ‚Üí `user_0001`
- Preserves behavioral patterns
- Auto-detects user/group columns
- Secure mapping file management

### üìä Comprehensive Analysis

- **Submission abandonment** - Do users avoid submitting during congestion?
- **True utilization** - Compare usage vs. actual capacity
- **Cloud bursting ROI** - Calculate hybrid cloud savings
- **Concurrent load** - Utilization time series
- **Multi-site comparison** - Cross-scheduler behavioral studies

### üìö Extensive Documentation

**40,000+ words** of documentation including:
- Scheduler-specific export guides
- Troubleshooting for every scheduler
- Privacy and anonymization workflows
- Institutional approval guidance
- Complete analysis tutorials

---

## üöÄ Quick Start

### For SLURM

```bash
# 1. Export cluster configuration
./export_slurm_cluster_config.sh

# 2. Export job history
./export_with_users.sh

# 3. Anonymize
./anonymize_cluster_data.sh \
  slurm_jobs_with_users.csv \
  slurm_anonymized.csv \
  mapping.txt

# 4. Analyze
python3 analyze_true_utilization.py slurm_anonymized.csv slurm_cluster_config.csv
python3 visualize_utilization.py utilization_timeseries.csv
python3 analyze_cross_user_impacts.py slurm_anonymized.csv slurm_cluster_config.csv
```

### For Other Schedulers

**LSF:**
```bash
./export_lsf_comprehensive.sh 2024/01/01 2024/12/31
```

**PBS:**
```bash
sudo ./export_pbs_comprehensive.sh 20240101 20241231
```

**UGE/SGE:**
```bash
./export_uge_comprehensive.sh 01/01/2024 12/31/2024
```

See [scheduler-specific guides](#documentation) for complete instructions.

---

## üìã Requirements

### System Requirements
- Bash 4.0+
- Python 3.6+
- Pandas (for analysis scripts)

### Scheduler Access
- Read access to scheduler accounting data
- Admin privileges may be required (PBS, LSF with `-a` flag)
- Scheduler-specific commands available:
  - SLURM: `sacct`, `sinfo`
  - LSF: `bhist`, `bhosts`
  - PBS: `pbsnodes`, accounting logs
  - UGE: `qacct`, `qhost`
  - HTCondor: `condor_history`, `condor_status`

---

## üìÅ Repository Structure

```
cluster-job-analysis/
‚îú‚îÄ‚îÄ README.md                           # This file
‚îÇ
‚îú‚îÄ‚îÄ Export Scripts/
‚îÇ   ‚îú‚îÄ‚îÄ export_slurm_cluster_config.sh  # SLURM cluster inventory
‚îÇ   ‚îú‚îÄ‚îÄ export_lsf_comprehensive.sh     # LSF complete export
‚îÇ   ‚îú‚îÄ‚îÄ export_lsf_cluster_config.sh    # LSF cluster inventory
‚îÇ   ‚îú‚îÄ‚îÄ export_pbs_comprehensive.sh     # PBS complete export
‚îÇ   ‚îú‚îÄ‚îÄ export_pbs_cluster_config.sh    # PBS cluster inventory
‚îÇ   ‚îú‚îÄ‚îÄ export_uge_comprehensive.sh     # UGE complete export
‚îÇ   ‚îú‚îÄ‚îÄ export_uge_cluster_config.sh    # UGE cluster inventory
‚îÇ   ‚îú‚îÄ‚îÄ export_htcondor_data.sh         # HTCondor export
‚îÇ   ‚îî‚îÄ‚îÄ export_cluster_configs_all.sh   # Auto-detect any scheduler
‚îÇ
‚îú‚îÄ‚îÄ Utilities/
‚îÇ   ‚îú‚îÄ‚îÄ anonymize_cluster_data.sh       # Privacy-preserving anonymization
‚îÇ   ‚îú‚îÄ‚îÄ standardize_cluster_config.py   # Normalize config formats
‚îÇ   ‚îî‚îÄ‚îÄ analyze_cluster_config.py       # Cluster capacity analysis
‚îÇ
‚îú‚îÄ‚îÄ Analysis Scripts/
‚îÇ   ‚îú‚îÄ‚îÄ analyze_true_utilization.py     # Comprehensive utilization analysis
‚îÇ   ‚îú‚îÄ‚îÄ visualize_utilization.py        # Publication-quality charts
‚îÇ   ‚îú‚îÄ‚îÄ analyze_job_resource_efficiency.py  # Job-level resource usage
‚îÇ   ‚îú‚îÄ‚îÄ analyze_queue_wait_times.py     # Wait time statistics & patterns
‚îÇ   ‚îú‚îÄ‚îÄ analyze_workload_characteristics.py # Arrivals, heavy-tail, correlations
‚îÇ   ‚îú‚îÄ‚îÄ analyze_short_jobs_and_arrays.py    # Workflow steps & scheduler overhead
‚îÇ   ‚îú‚îÄ‚îÄ analyze_cross_user_impacts.py   # Submission abandonment triggers
‚îÇ   ‚îú‚îÄ‚îÄ analyze_concurrent_load.py      # Utilization over time (legacy)
‚îÇ   ‚îú‚îÄ‚îÄ analyze_submission_abandonment_events.py  # Test hypothesis (legacy)
‚îÇ   ‚îú‚îÄ‚îÄ analyze_jobs.py                 # Basic statistics
‚îÇ   ‚îú‚îÄ‚îÄ analyze_utilization.py          # Utilization metrics (legacy)
‚îÇ   ‚îú‚îÄ‚îÄ analyze_packing.py              # Node packing efficiency
‚îÇ   ‚îú‚îÄ‚îÄ analyze_slurm_cloud_bursting.py # Cloud ROI
‚îÇ   ‚îî‚îÄ‚îÄ analyze_full_aws_migration.py   # Full migration analysis
‚îÇ
‚îú‚îÄ‚îÄ Documentation/
‚îÇ   ‚îú‚îÄ‚îÄ COMPLETE_TOOLKIT_SUMMARY.md     # Master overview
‚îÇ   ‚îú‚îÄ‚îÄ LSF_EXPORT_GUIDE.md             # LSF detailed guide
‚îÇ   ‚îú‚îÄ‚îÄ PBS_EXPORT_GUIDE.md             # PBS detailed guide
‚îÇ   ‚îú‚îÄ‚îÄ UGE_EXPORT_GUIDE.md             # UGE/SGE detailed guide
‚îÇ   ‚îú‚îÄ‚îÄ MULTI_SCHEDULER_EXPORT_README.md # All schedulers reference
‚îÇ   ‚îú‚îÄ‚îÄ TWO_PART_EXPORT_GUIDE.md        # Why config + jobs matters
‚îÇ   ‚îú‚îÄ‚îÄ ANONYMIZATION_README.md         # Privacy workflow
‚îÇ   ‚îú‚îÄ‚îÄ INSTITUTIONAL_APPROVAL_CASE.md  # Getting approval
‚îÇ   ‚îú‚îÄ‚îÄ OSCAR_Cluster_Analysis.md       # Example analysis
‚îÇ   ‚îî‚îÄ‚îÄ CLOUD_COMPARISON_ANALYSIS.md    # Cloud economics
‚îÇ
‚îî‚îÄ‚îÄ Testing/
    ‚îú‚îÄ‚îÄ test_anonymization.sh           # Validate anonymization
    ‚îú‚îÄ‚îÄ generate_mock_user_group_data.py # Test data generation
    ‚îî‚îÄ‚îÄ generate_sample_data_with_users.sh # Sample data
```

---

## üìä Analysis Capabilities

### Core Analysis Tools

#### 1. True Utilization Analysis
**Script:** `analyze_true_utilization.py`

Calculates cluster utilization at every job transition point (start/end):
- Node-level metrics (busy/idle status)
- CPU-level metrics (allocated vs capacity)
- Memory-level metrics (allocated vs capacity)
- Statistics: mean, median, percentiles (25th, 75th, 90th, 95th, 99th)
- Time-weighted averages (accounts for duration)
- Time series output for visualization

**Usage:**
```bash
python3 analyze_true_utilization.py jobs_anon.csv cluster_config.csv
# Outputs: utilization_timeseries.csv, utilization_statistics.csv
```

**Documentation:** See [UTILIZATION_ANALYSIS_GUIDE.md](UTILIZATION_ANALYSIS_GUIDE.md)

#### 2. Utilization Visualization
**Script:** `visualize_utilization.py`

Creates publication-quality charts (300 DPI):
- CPU utilization timeline with mean/median lines
- Memory utilization timeline
- Combined CPU+Memory dual-panel chart
- Distribution histograms
- Daily average aggregations

**Usage:**
```bash
python3 visualize_utilization.py utilization_timeseries.csv
# Outputs: 5 PNG charts
```

#### 3. Job Resource Efficiency
**Script:** `analyze_job_resource_efficiency.py`

Analyzes requested vs. actually used resources:
- CPU efficiency: (used / requested) √ó 100%
- Memory efficiency: (used / requested) √ó 100%
- GPU efficiency (if available)
- Per-job, per-user, per-group statistics
- Weekly trends and temporal patterns

**Outputs:** 4 CSV files with efficiency metrics

#### 4. Queue Wait Time Analysis
**Script:** `analyze_queue_wait_times.py`

Comprehensive wait time statistics with temporal patterns:
- Time of day (24 hours)
- Day of week (7 days)
- Week of month (1-5)
- Month of year (12 months)
- Per-user and per-group statistics
- Job size correlation analysis
- Calendar time series

**Outputs:** 8 CSV files with wait time patterns

#### 5. Workload Characterization
**Script:** `analyze_workload_characteristics.py`

Statistical workload analysis:
- **Arrival patterns**: Poisson testing, inter-arrival times
- **Burstiness**: Index calculation (œÉ¬≤ - Œº) / (œÉ¬≤ + Œº)
- **Group correlation**: Cross-correlation matrix
- **Heavy-tailed distribution**: Power law exponent (Pareto Œ±)
- **Autocorrelation**: Multiple time lags (1h, 2h, 6h, 12h, 24h, 48h, 1week)
- Jobs > 95th percentile runtime
- % of CPU-hours consumed by heavy-tail jobs

**Outputs:** 5 CSV files with statistical analysis

#### 6. Short Jobs & Array Analysis
**Script:** `analyze_short_jobs_and_arrays.py`

Identifies workflow patterns and scheduler overhead:
- Short runtime jobs (30s, 1min, 5min, 15min, 1hr thresholds)
- Job array detection (rapid successive submissions)
- Workflow sequence detection (chained dependent jobs)
- Per-user and per-group patterns
- Temporal distributions
- **Scheduler overhead impact estimates**
- System efficiency implications

**Outputs:** 7 CSV files with job patterns and overhead analysis

#### 7. Cross-User Impact Analysis
**Script:** `analyze_cross_user_impacts.py`

**üî• NEW - Enhanced Submission Abandonment Study**

Analyzes how individual user behaviors impact others and trigger submission abandonment:

**Key Features:**
- Identifies high-impact jobs (large resources, long runtimes)
- Measures before/after effects on other users:
  - Submission rate changes
  - Wait time increases
  - Active user count changes
- Detects **submission abandonment triggers**
- Identifies recurring patterns (e.g., "User X every Tuesday")
- Temporal analysis of when impacts occur
- Cross-group impact relationships

**Example Insights Enabled:**
- "User X's Tuesday batch jobs cause 2x wait time increase for others"
- "When User Y submits >1000 core jobs, submission rate drops 40% within 1 hour"
- "Group A's monthly jobs trigger submission abandonment in Group B"

**Usage:**
```bash
python3 analyze_cross_user_impacts.py jobs_anon.csv cluster_config.csv
```

**Outputs:** 6 CSV files including:
- `high_impact_jobs.csv` - Jobs with significant system impact
- `impact_events.csv` - Specific incidents with before/after metrics
- `user_impact_patterns.csv` - Recurring patterns per user
- `submission_abandonment_triggers.csv` - Events that trigger abandonment
- `temporal_impact_patterns.csv` - Day/time patterns of impacts
- `cross_group_impacts.csv` - How groups affect each other

**Provides both:**
- ‚úÖ General patterns over time
- ‚úÖ Specific data points (individual incidents)
- ‚úÖ Causal relationships (User X ‚Üí System Y ‚Üí Others Z)

---

## üéì Use Cases

### 1. Submission Abandonment Study

**Hypothesis:** Users avoid submitting jobs when the queue is long.

**Process:**
1. Export job history from 10-20 HPC sites
2. Anonymize all user data
3. Analyze correlation between queue depth and submission rate
4. Compare across different utilization levels

**Expected Results:**
- Overprovisioned clusters: weak effect (r ‚âà -0.1)
- Normally loaded: moderate effect (r ‚âà -0.2 to -0.3)
- Underprovisioned (4-7x): strong effect (r ‚âà -0.4 to -0.5)

### 2. True Utilization Calculation

**Problem:** Job data only shows nodes that ran jobs, not total capacity.

**Solution:**
1. Export cluster configuration (all nodes)
2. Export job history (what was used)
3. Calculate: `utilization = peak_usage / total_capacity`

**Example:**
```
Cluster capacity: 13,920 CPUs (from config)
Peak concurrent:  12,500 CPUs (from jobs)
True utilization: 89.8%
```

### 3. Cloud Bursting ROI

**Analysis:**
1. Identify peak usage periods
2. Calculate overflow beyond on-prem capacity
3. Price AWS spot instances for overflow
4. Compare cost vs. expanding on-prem

**Example Finding:**
- OSCAR: $7.4M savings over 3 years with cloud bursting vs. 2x cluster expansion

### 4. Cross-Scheduler Comparison

**Question:** Do users behave differently on different schedulers?

**Process:**
1. Export from multiple sites with different schedulers
2. Standardize all formats
3. Compare submission patterns, utilization, abandonment behavior
4. Control for utilization level and workload type

### 5. Multi-Site Behavioral Studies

**Goal:** Understand HPC usage patterns across the ecosystem

**Requirements:**
- 10-20 sites
- Mixed schedulers (SLURM, LSF, PBS, UGE)
- Mixed utilization (overprovisioned to heavily contended)
- Mixed scales (100 to 10,000+ nodes)

---

## üìñ Documentation

### Getting Started
- **[COMPLETE_TOOLKIT_SUMMARY.md](COMPLETE_TOOLKIT_SUMMARY.md)** - Complete overview of all tools
- **[MULTI_SCHEDULER_EXPORT_README.md](MULTI_SCHEDULER_EXPORT_README.md)** - Quick reference for all schedulers
- **[TWO_PART_EXPORT_GUIDE.md](TWO_PART_EXPORT_GUIDE.md)** - Why you need config + job data

### Scheduler-Specific Guides
- **[LSF_EXPORT_GUIDE.md](LSF_EXPORT_GUIDE.md)** - Complete IBM Spectrum LSF guide
- **[PBS_EXPORT_GUIDE.md](PBS_EXPORT_GUIDE.md)** - Complete PBS/Torque guide
- **[UGE_EXPORT_GUIDE.md](UGE_EXPORT_GUIDE.md)** - Complete UGE/SGE guide

### Process Guides
- **[ANONYMIZATION_README.md](ANONYMIZATION_README.md)** - Privacy and anonymization
- **[INSTITUTIONAL_APPROVAL_CASE.md](INSTITUTIONAL_APPROVAL_CASE.md)** - Getting institutional approval

### Analysis Examples
- **[OSCAR_Cluster_Analysis.md](OSCAR_Cluster_Analysis.md)** - Brown University OSCAR analysis
- **[CLOUD_COMPARISON_ANALYSIS.md](CLOUD_COMPARISON_ANALYSIS.md)** - Cloud economics comparison
- **[USER_GROUP_ANALYSIS_OPPORTUNITIES.md](USER_GROUP_ANALYSIS_OPPORTUNITIES.md)** - Behavioral analysis ideas

---

## üî¨ Example Results

### OSCAR Cluster (Brown University)

**Dataset:** 6,991,376 jobs over ~1 year

**Key Findings:**
- Median queue time: 1.93 minutes (overprovisioned!)
- Peak concurrent: 14,645 CPUs
- Mean concurrent: 7,883 CPUs
- **Submission abandonment detected:** r = -0.09 (weak but present)

**Conclusion:** Even on an overprovisioned cluster where queue checking provides minimal benefit, users still exhibit abandonment behavior. On heavily contended clusters, this effect would be much stronger.

**Business Impact:** Cloud bursting could save $7.4M over 3 years vs. expanding on-prem capacity by 2x.

---

## üîê Privacy & Security

### Anonymization

The toolkit provides **deterministic, privacy-preserving anonymization**:

**What gets anonymized:**
- ‚úÖ Usernames ‚Üí `user_0001`, `user_0002`, ...
- ‚úÖ Groups ‚Üí `group_A`, `group_B`, ...
- ‚úÖ Accounts ‚Üí `account_X`, `account_Y`, ...

**What stays the same:**
- ‚úÖ All timestamps (submit, start, end)
- ‚úÖ All resource requests (CPUs, memory)
- ‚úÖ All usage patterns
- ‚úÖ Job dependencies and sequences

**Mapping file security:**
```bash
# Mapping file links anonymous ‚Üî real identities
chmod 600 mapping_secure.txt
sudo mv mapping_secure.txt /root/secure/

# Never share mapping file
# Only share anonymized CSV
```

### Data Handling Best Practices

1. **Export** on secure system (scheduler head node)
2. **Anonymize** immediately after export
3. **Secure** mapping file (root-only access)
4. **Delete** original export with real usernames
5. **Encrypt** for transfer (GPG or SSH)
6. **Share** only anonymized data

See [ANONYMIZATION_README.md](ANONYMIZATION_README.md) for complete workflow.

---

## üåç Supported Schedulers

### SLURM (Simple Linux Utility for Resource Management)

**Common in:** Universities, national labs, cloud HPC
**Commands:** `sacct`, `sinfo`
**Date format:** `YYYY-MM-DD`
**Output:** Native CSV
**Guide:** Built-in documentation + [MULTI_SCHEDULER_EXPORT_README.md](MULTI_SCHEDULER_EXPORT_README.md)

### LSF (IBM Spectrum LSF)

**Common in:** National labs, commercial HPC, financial services
**Commands:** `bhist`, `bhosts`, `lshosts`
**Date format:** `YYYY/MM/DD`
**Output:** Text paragraphs ‚Üí parsed to CSV
**Guide:** [LSF_EXPORT_GUIDE.md](LSF_EXPORT_GUIDE.md)

### PBS/Torque/PBS Pro

**Common in:** Universities, DOD, research institutions
**Commands:** `pbsnodes`, accounting logs
**Date format:** `YYYYMMDD`
**Output:** Semicolon-delimited logs ‚Üí parsed to CSV
**Guide:** [PBS_EXPORT_GUIDE.md](PBS_EXPORT_GUIDE.md)
**Note:** Requires root/admin access for accounting logs

### UGE/SGE/OGE (Univa/Sun/Open Grid Engine)

**Common in:** Legacy installations, academic clusters
**Commands:** `qacct`, `qhost`
**Date format:** `MM/DD/YYYY` ‚ö†Ô∏è **Unique!**
**Output:** Text with separators ‚Üí parsed to CSV
**Guide:** [UGE_EXPORT_GUIDE.md](UGE_EXPORT_GUIDE.md)

### HTCondor

**Common in:** High-throughput computing, OSG
**Commands:** `condor_history`, `condor_status`
**Date format:** Days ago (integer)
**Output:** Tab-separated ‚Üí converted to CSV
**Guide:** [MULTI_SCHEDULER_EXPORT_README.md](MULTI_SCHEDULER_EXPORT_README.md)

---

## üí° Tips & Best Practices

### Exporting Data

1. **Start small** - Test with 1 month of data first
2. **Check permissions** - Some schedulers require admin access
3. **Verify output** - Always check CSV before anonymization
4. **Export both parts** - Cluster config + job history
5. **Document metadata** - Record scheduler version, date range, site info

### Anonymization

1. **Anonymize immediately** - Don't leave raw data sitting around
2. **Secure mapping file** - Root-only access, encrypted backups
3. **Verify anonymization** - Check no real usernames remain
4. **Delete originals** - Remove raw export after anonymization
5. **Test first** - Use test script on sample data

### Analysis

1. **Validate data** - Check for reasonable values, complete timestamps
2. **Start with basics** - Run `analyze_jobs.py` first
3. **Understand capacity** - Always analyze cluster config
4. **Compare thoughtfully** - Control for utilization, workload, scale
5. **Document assumptions** - Note data quality issues, filters applied

---

## ü§ù Contributing

This toolkit is designed for HPC research and operations. Contributions welcome:

- **Bug reports** - Scheduler-specific parsing issues
- **New schedulers** - Add support for additional schedulers
- **Analysis scripts** - New analytical approaches
- **Documentation** - Improved guides, examples
- **Test cases** - Validation on different environments

---

## üìÑ License

MIT License - See LICENSE file for details

---

## üôè Acknowledgments

**Data Source:**
- Brown University OSCAR cluster data used for initial validation

**Schedulers:**
- SLURM by SchedMD
- IBM Spectrum LSF
- Altair PBS Pro / OpenPBS / Torque
- Univa Grid Engine / Sun Grid Engine / Open Grid Engine
- HTCondor by University of Wisconsin-Madison

---

## üìû Support

### Documentation
Start with [COMPLETE_TOOLKIT_SUMMARY.md](COMPLETE_TOOLKIT_SUMMARY.md) for comprehensive overview.

### Scheduler-Specific Issues
See scheduler guides:
- [LSF_EXPORT_GUIDE.md](LSF_EXPORT_GUIDE.md)
- [PBS_EXPORT_GUIDE.md](PBS_EXPORT_GUIDE.md)
- [UGE_EXPORT_GUIDE.md](UGE_EXPORT_GUIDE.md)

### Common Issues

**"Command not found"**
- Source scheduler environment file
- Check scheduler installation path

**"Permission denied"**
- Some schedulers require admin access
- See scheduler guide for fallback options

**"No data exported"**
- Verify date format for your scheduler
- Check accounting is enabled
- Confirm date range has data

**Troubleshooting checklists** in each scheduler guide.

---

## üéØ Project Goals

1. **Enable behavioral research** - Understand how users interact with HPC schedulers
2. **Test submission abandonment** - Quantify avoidance behavior during congestion
3. **Improve resource efficiency** - Data-driven provisioning decisions
4. **Support cloud adoption** - ROI analysis for hybrid cloud
5. **Cross-site comparison** - Learn from ecosystem-wide patterns

---

## üìä Project Status

**Current:** Production ready, all 5 major schedulers supported

**Next Steps:**
- Multi-site data collection (target: 10-20 sites)
- Cross-scheduler behavioral comparison
- Publication of findings
- Cloud bursting ROI calculator web tool

---

## üöÄ Getting Started Checklist

- [ ] Choose your scheduler from [supported list](#supported-schedulers)
- [ ] Read scheduler-specific [export guide](#documentation)
- [ ] Set up scheduler environment (PATH, environment variables)
- [ ] Test export on small date range (1 month)
- [ ] Verify CSV output looks correct
- [ ] Export full dataset (6-12 months recommended)
- [ ] Export cluster configuration
- [ ] Anonymize data using `anonymize_cluster_data.sh`
- [ ] Secure mapping file (root-only access)
- [ ] Run `analyze_jobs.py` for basic validation
- [ ] Run `analyze_concurrent_load.py` for utilization
- [ ] Run `analyze_submission_abandonment_events.py` to test hypothesis
- [ ] Review [analysis documentation](#documentation)
- [ ] Share findings (anonymized data only!)

---

**Ready to analyze your HPC cluster?** Start with your scheduler's [export guide](#documentation)! üìà
